{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\CSCLDL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Union, Tuple, List, Iterable, Dict\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "#from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import gzip, csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.init as init\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        #print(\"IN POSTIONAL FORWARD\")\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def scaled_dot_product(q, k, v, attn_drop_rate=0.1, mask=None):\n",
    "def scaled_dot_product(q, k, v, attn_drop_rate=0.1):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      q: query, shape: (batch, # heads, seq len, head dimension)\n",
    "      k: keys, shape: (batch, # heads, seq len, head dimension)\n",
    "      v: value, shape: (batch, # heads, seq len, head dimension)\n",
    "      attn_drop_rate: probability of an element to be zeroed,\n",
    "      mask: the optional masking of specific entries in the attention matrix.\n",
    "              shape: (batch, seq len)\n",
    "    \"\"\"\n",
    "    \n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-1, -2))\n",
    "    attn_logits = attn_logits/math.sqrt(d_k)\n",
    "    # if mask is not None:\n",
    "    #   dummy_mask = torch.where(mask == 1.0, torch.tensor(True), torch.tensor(False))\n",
    "    #   attn_logits = attn_logits.masked_fill(dummy_mask.unsqueeze(1).unsqueeze(1), float('-inf'))\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    attention = F.dropout(attention)\n",
    "    values = torch.matmul(attention,v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.query = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "      nn.init.xavier_uniform_(self.query.weight)\n",
    "      self.query.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.key.weight)\n",
    "      self.key.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.value.weight)\n",
    "      self.value.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "      self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def split_heads(self, tensor):\n",
    "       new_shape = tensor.size()[:-1] + (self.n_heads, self.head_dim)\n",
    "       tensor = tensor.view(*new_shape)\n",
    "       tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "       return tensor\n",
    "    \n",
    "    def merge_heads(self, tensor, batch_size, seq_length):\n",
    "       tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "       return tensor\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "       print(\"ATTENTION\",embedding.size())\n",
    "       batch_size, seq_length, embed_dim = embedding.size()\n",
    "       q, k, v = self.query(embedding), self.key(embedding), self.value(embedding)\n",
    "       q = self.split_heads(q)\n",
    "       k = self.split_heads(k)\n",
    "       v = self.split_heads(v)\n",
    "       values = scaled_dot_product(q, k, v, self.attn_drop_rate)\n",
    "       values = self.merge_heads(values, batch_size, seq_length)\n",
    "       attended_embeds = self.o_proj(values)\n",
    "\n",
    "       return attended_embeds\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        #print(f\"Mean ({mean.size()})\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        #print(f\"Standard Deviation  ({std.size()})\")\n",
    "        y = (inputs - mean) / std\n",
    "        #print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y  + self.beta\n",
    "        #print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
    "        #print(f\"out: {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, 4*embed_dim)\n",
    "        self.linear2 = nn.Linear(4*embed_dim, embed_dim)\n",
    "        self.relu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        #print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier: #mewmew\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, numclasses, dropout_rate=0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        # self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(input_dim, numclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads, attn_drop_rate)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout1 = nn.Dropout(p=layer_drop_rate)\n",
    "        self.ffn = PositionwiseFeedForward(self.embed_dim,layer_drop_rate)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout2 = nn.Dropout(p=layer_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"IN ENCODER FORWARD\",x.shape, x.size())\n",
    "        residual_x = x\n",
    "        #print(\"------- ATTENTION 1 ------\")\n",
    "        x = self.attention(x)\n",
    "        #print(\"------- DROPOUT 1 ------\")\n",
    "        x = self.dropout1(x)\n",
    "        #print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
    "        x = x + residual_x\n",
    "        x = self.norm1(x)\n",
    "        residual_x = x\n",
    "        #print(\"------- ATTENTION 2 ------\")\n",
    "        x = self.ffn(x)\n",
    "        #print(\"------- DROPOUT 2 ------\")\n",
    "        x = self.dropout2(x)\n",
    "        #print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
    "        x = x + residual_x\n",
    "        x = self.norm2(x)\n",
    "        #add and norm switch refer to assignment\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN ENCODER FORWARD torch.Size([3, 2, 16]) torch.Size([3, 2, 16])\n",
      "------- ATTENTION 1 ------\n",
      "ATTENTION torch.Size([3, 2, 16])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([3, 2, 64])\n",
      "x after 2nd linear layer: torch.Size([3, 2, 16])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "Output shape:  (3, 2, 16)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16\n",
    "n_heads = 4\n",
    "attn_drop_rate = 0.1\n",
    "layer_drop_rate = 0.1\n",
    "block = EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n",
    "\n",
    "bs = 3\n",
    "seq_len = 2\n",
    "embeds = torch.randn(bs, seq_len, embed_dim)\n",
    "outputs = block(embeds)\n",
    "out_bs, out_seq_len, out_hidden = outputs.shape\n",
    "print(\"Output shape: \", (out_bs, out_seq_len, out_hidden))\n",
    "assert out_bs == bs and out_seq_len == seq_len and out_hidden == embed_dim, \"Unexpected output shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module): #Transformer\n",
    "    def __init__(self, n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+1, embed_dim)\n",
    "        self.position = PositionalEncoding(embed_dim, layer_drop_rate)\n",
    "        self.net = nn.Sequential(*[\n",
    "        EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate) for _ in range(n_layers)\n",
    "        ])\n",
    "        #self.mask_pred = nn.Linear(embed_dim, vocab_size) ## Classifier\n",
    "        self.pooler = nn.Sequential(OrderedDict([\n",
    "            ('dense', nn.Linear(embed_dim, embed_dim)),\n",
    "            ('activation', nn.Tanh()),\n",
    "        ]))\n",
    "        self.classifier = Classifier(embed_dim, num_classes)\n",
    "    def forward(self, batch_text):\n",
    "        #print(\"in model forward\", batch_text.size())\n",
    "        embedding = self.position(self.embed(batch_text))\n",
    "        #print(\"POST Postional\", embedding.size())\n",
    "        new_embedding = self.net((embedding))\n",
    "        #print(\"POST ENCODERS\", new_embedding.size())\n",
    "        o = self.pooler(new_embedding[:, 0])\n",
    "        #print(o.shape)\n",
    "        preds = self.classifier(o)\n",
    "        return preds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in model forward torch.Size([3, 2])\n",
      "IN POSTIONAL FORWARD\n",
      "POST Postional torch.Size([3, 2, 16])\n",
      "IN ENCODER FORWARD torch.Size([3, 2, 16]) torch.Size([3, 2, 16])\n",
      "------- ATTENTION 1 ------\n",
      "ATTENTION torch.Size([3, 2, 16])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([3, 2, 64])\n",
      "x after 2nd linear layer: torch.Size([3, 2, 16])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "IN ENCODER FORWARD torch.Size([3, 2, 16]) torch.Size([3, 2, 16])\n",
      "------- ATTENTION 1 ------\n",
      "ATTENTION torch.Size([3, 2, 16])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([3, 2, 64])\n",
      "x after 2nd linear layer: torch.Size([3, 2, 16])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "POST ENCODERS torch.Size([3, 2, 16])\n",
      "torch.Size([3, 16])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "vocab_size = 10\n",
    "attn_drop_rate = 0.1\n",
    "layer_drop_rate = 0.1\n",
    "num_classes=20\n",
    "model = BERT(n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate)\n",
    "\n",
    "bs = 3\n",
    "seq_len = 2\n",
    "inputs = torch.randint(0, vocab_size, (bs, seq_len))\n",
    "mask_preds = model(inputs)\n",
    "print(mask_preds.shape)\n",
    "#out_bs, out_seq_len, out_vocab = mask_preds.shape\n",
    "#print(\"Mask predictions shape: \", (out_bs, out_seq_len, out_vocab))\n",
    "#print(mask_preds)\n",
    "#assert out_bs == bs and out_seq_len == seq_len and out_vocab == vocab_size, \"Unexpected mask prediction output shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-9.1718e-01, -1.3727e+00,  2.2097e-01, -9.8661e-02,  2.9791e-01,\n",
      "           4.8143e-01,  7.2774e-01, -2.5986e-01,  2.4598e-01,  4.4886e-01,\n",
      "          -3.7723e-01, -2.0071e-01,  4.8600e-01, -7.5484e-01,  6.2974e-01,\n",
      "           2.2242e-01,  1.5447e+00,  1.2410e+00,  1.8637e-01,  5.9945e-01],\n",
      "         [ 4.1521e-01, -4.8066e-01,  3.6566e-01,  7.2611e-01,  4.3182e-01,\n",
      "          -5.3013e-02,  6.6377e-01,  3.5730e-01, -1.0911e-02,  5.7580e-01,\n",
      "          -9.6048e-01, -1.9430e-01,  1.5048e+00,  5.2852e-01, -2.9945e-01,\n",
      "           1.5301e-01,  7.9827e-01,  6.5249e-01,  1.4291e+00, -3.6569e-01]],\n",
      "\n",
      "        [[ 3.5886e-01,  1.4730e+00,  2.4403e-01,  2.8965e-01,  3.7539e-01,\n",
      "          -7.6075e-01,  3.1571e-01,  6.6394e-02, -3.5843e-01, -2.1372e-01,\n",
      "          -1.0680e+00, -3.1656e-01,  9.7150e-01, -1.2280e-03, -6.9403e-01,\n",
      "          -3.9616e-01, -9.4375e-02, -3.1369e-01,  4.3219e-01, -4.6102e-01],\n",
      "         [-9.2675e-01, -4.7492e-01, -4.2623e-01,  1.0391e-01,  8.5269e-01,\n",
      "           9.0149e-01,  1.1164e+00, -2.3395e-01,  8.8189e-01,  3.8932e-01,\n",
      "           1.7568e-01, -9.9411e-01,  8.6315e-01,  2.8450e-01, -1.0562e-01,\n",
      "          -2.6773e-01,  2.7238e-01,  8.9970e-01,  9.9530e-02, -5.3563e-01]],\n",
      "\n",
      "        [[-3.2555e-03,  6.1914e-01,  6.2064e-01, -4.0962e-01, -1.1972e-01,\n",
      "          -1.3933e+00,  5.9664e-01,  1.5531e-01, -3.9692e-01,  5.0672e-01,\n",
      "          -1.1619e+00,  1.1739e-01,  1.2261e+00, -7.0174e-01, -1.4481e-01,\n",
      "          -1.7766e-01,  2.1744e-01,  9.9310e-02,  1.8153e-01,  1.9497e-01],\n",
      "         [ 7.6541e-02,  2.1184e-01,  9.0285e-01, -1.8725e-01, -3.6424e-02,\n",
      "          -4.9005e-01,  5.0238e-01,  5.0107e-01, -6.3931e-01,  1.2213e-01,\n",
      "          -1.0060e+00,  4.3313e-01,  3.1913e-01, -9.8419e-02,  3.6493e-01,\n",
      "          -1.0807e-01,  1.5703e+00,  3.1126e-01,  4.9583e-01,  8.8533e-02]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mask_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Replace with your desired tokenizer\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.tokenizer(item['text'], truncation=True, padding='max_length', return_tensors='pt', max_length=512)\n",
    "        encoding['label'] = torch.tensor(item['label'])\n",
    "        return encoding\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"setfit/20_newsgroups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"setfit/20_newsgroups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = MyDataset(dataset=dataset['train'], tokenizer=tokenizer)\n",
    "testdata = MyDataset(dataset=dataset['test'], tokenizer=tokenizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(traindata, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(traindata, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in model forward torch.Size([32, 512])\n",
      "IN POSTIONAL FORWARD\n",
      "POST Postional torch.Size([32, 512, 512])\n",
      "IN ENCODER FORWARD torch.Size([32, 512, 512]) torch.Size([32, 512, 512])\n",
      "------- ATTENTION 1 ------\n",
      "ATTENTION torch.Size([32, 512, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([32, 512, 1]))\n",
      "Standard Deviation  (torch.Size([32, 512, 1]))\n",
      "y: torch.Size([32, 512, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([32, 512, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([32, 512, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 512, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([32, 512, 1]))\n",
      "Standard Deviation  (torch.Size([32, 512, 1]))\n",
      "y: torch.Size([32, 512, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([32, 512, 512])\n",
      "IN ENCODER FORWARD torch.Size([32, 512, 512]) torch.Size([32, 512, 512])\n",
      "------- ATTENTION 1 ------\n",
      "ATTENTION torch.Size([32, 512, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([32, 512, 1]))\n",
      "Standard Deviation  (torch.Size([32, 512, 1]))\n",
      "y: torch.Size([32, 512, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([32, 512, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([32, 512, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 512, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([32, 512, 1]))\n",
      "Standard Deviation  (torch.Size([32, 512, 1]))\n",
      "y: torch.Size([32, 512, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([32, 512, 512])\n",
      "POST ENCODERS torch.Size([32, 512, 512])\n",
      "torch.Size([32, 512])\n",
      "HELLO torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 512\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "vocab_size = 30522\n",
    "attn_drop_rate = 0.1\n",
    "layer_drop_rate = 0.1\n",
    "num_classes=20\n",
    "model = BERT(n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate)\n",
    "num_epochs =1\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        labels = batch['label']\n",
    "        output = model(input_ids)\n",
    "\n",
    "        print(\"HELLO\",output.shape)\n",
    "        loss = criterion(output, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0595, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2513e-01, -1.9240e-02, -1.1992e-01,  1.9925e-01, -1.1545e-01,\n",
       "         -1.6353e-01,  2.2236e-01,  3.9057e-01, -2.3930e-01, -4.1146e-01,\n",
       "          1.4754e-01,  3.1831e-01, -3.4704e-01,  1.6187e-01,  2.0630e-01,\n",
       "          4.2818e-01, -5.7798e-01,  3.3938e-01, -6.6719e-02,  1.7823e-01],\n",
       "        [-2.8443e-01, -1.1484e-01, -8.4109e-02,  9.7996e-02, -1.9036e-01,\n",
       "         -3.6507e-01,  4.6729e-01,  1.9681e-01, -1.2402e-01, -4.0387e-01,\n",
       "          1.1493e-01,  1.0444e-01, -6.1762e-02,  2.8009e-01,  9.5937e-02,\n",
       "          3.2025e-01, -5.4876e-01,  3.6004e-01, -2.3330e-01,  1.0138e-01],\n",
       "        [-1.6718e-01, -2.0669e-01, -2.3657e-01,  9.6869e-03, -2.1960e-01,\n",
       "         -3.2149e-01,  4.2171e-01,  8.0840e-02, -1.5137e-01, -4.6460e-01,\n",
       "          1.3797e-01,  2.3909e-01,  4.9093e-02,  2.0537e-01,  4.0825e-02,\n",
       "          3.0688e-01, -5.0838e-01,  2.3719e-01, -3.1818e-01,  2.0476e-01],\n",
       "        [ 4.8505e-02,  1.4172e-01, -9.6209e-02, -2.5069e-01, -1.8217e-01,\n",
       "          9.8549e-02,  2.6542e-01,  4.1043e-01, -3.2826e-01, -3.4900e-01,\n",
       "          1.6321e-01,  1.3554e-02,  1.9474e-01, -3.7561e-02,  1.5907e-01,\n",
       "          3.0301e-01, -3.4090e-01,  2.9891e-01,  4.1512e-01,  4.2363e-01],\n",
       "        [-4.8934e-02, -8.3378e-02, -1.5262e-02, -6.0688e-02, -8.5779e-02,\n",
       "         -3.3825e-02,  4.7565e-01,  1.8689e-01, -3.2386e-01, -2.4955e-01,\n",
       "          4.3046e-01,  2.5274e-01,  8.0505e-02,  9.3018e-02,  2.5806e-01,\n",
       "          2.1701e-01, -6.6059e-01,  2.6988e-01, -1.6990e-01,  8.0373e-02],\n",
       "        [ 1.3799e-01, -2.3243e-02, -1.9165e-01, -1.9389e-02, -4.7784e-02,\n",
       "         -8.0604e-02,  3.3641e-01,  4.6819e-01, -1.7079e-01, -3.4143e-01,\n",
       "          2.0811e-01,  1.8713e-01, -4.7776e-02,  1.4219e-01,  2.5536e-01,\n",
       "          4.0170e-01, -3.8761e-01,  3.2645e-01,  1.3401e-01,  2.7211e-01],\n",
       "        [ 1.5507e-01, -1.5531e-01,  5.4861e-02,  8.8424e-02, -6.5087e-02,\n",
       "          3.5388e-02,  4.0347e-01,  6.4094e-02, -3.8017e-02, -3.0844e-01,\n",
       "          2.3383e-01,  3.0502e-01, -9.6478e-02,  4.9632e-01, -6.6259e-02,\n",
       "          2.0564e-01, -4.4282e-01, -1.0187e-02, -9.2432e-02,  2.3417e-01],\n",
       "        [-2.0413e-01, -9.6434e-02, -6.4098e-02,  6.4991e-02, -2.7731e-01,\n",
       "         -5.5011e-02,  3.9690e-01,  9.1129e-02,  1.6784e-01, -3.1313e-01,\n",
       "          1.7462e-01,  9.0345e-02, -2.9822e-01,  6.2381e-01,  1.3712e-01,\n",
       "          3.6386e-01, -5.5466e-01, -2.7618e-02, -1.4846e-01,  1.0848e-01],\n",
       "        [-2.5347e-01, -1.7845e-01, -1.2048e-01,  2.9399e-01, -2.1754e-01,\n",
       "         -2.5976e-01,  3.7344e-01,  3.4602e-02, -1.9149e-01, -3.2490e-01,\n",
       "          7.6973e-02,  2.2745e-01, -1.8565e-01,  3.6910e-01, -1.3115e-01,\n",
       "          4.2687e-01, -6.2368e-01,  2.3412e-01, -8.1712e-02,  2.5674e-01],\n",
       "        [-2.6953e-01, -1.4883e-01, -2.0090e-01,  1.3474e-01, -5.1488e-02,\n",
       "         -2.2662e-01,  1.0858e-01,  4.0050e-01, -4.0126e-01, -6.1039e-01,\n",
       "          2.0041e-01,  2.3959e-01, -1.3632e-01,  3.3404e-01,  2.0244e-01,\n",
       "          4.0063e-01, -5.9794e-01,  2.0349e-01,  1.6818e-01,  4.4382e-01],\n",
       "        [-1.2524e-01, -6.2474e-02, -5.9529e-02,  1.2402e-01, -1.4408e-01,\n",
       "          1.7262e-03,  4.8059e-01,  1.0629e-02, -2.8334e-01, -2.7422e-01,\n",
       "          2.9230e-01,  3.3827e-01, -2.5588e-03,  5.4374e-01,  1.3499e-01,\n",
       "          4.2010e-01, -5.8842e-01,  1.1623e-01, -1.6170e-01,  4.2939e-02],\n",
       "        [-1.9955e-02, -9.3059e-02, -2.7536e-01, -8.8779e-02, -3.1945e-02,\n",
       "          1.3826e-01,  3.5922e-01,  4.5885e-01, -2.8621e-01, -3.7487e-01,\n",
       "          1.0853e-01,  1.4726e-02, -1.9901e-01,  1.2400e-01,  1.3392e-01,\n",
       "          3.1313e-01, -3.8852e-01,  3.5107e-01,  3.6397e-01,  2.0016e-01],\n",
       "        [-1.4719e-01, -4.1915e-03,  7.4506e-02, -1.2610e-03, -2.8312e-01,\n",
       "         -5.4458e-02,  2.9813e-01,  2.7846e-01,  1.0183e-01, -4.3893e-01,\n",
       "          3.3586e-02,  2.9606e-01, -1.4656e-01,  1.7522e-01,  2.3949e-01,\n",
       "          4.0544e-01, -3.7686e-01,  6.4738e-01,  2.9381e-01,  3.4796e-01],\n",
       "        [-2.6018e-02, -2.2168e-01,  1.3921e-01,  3.3926e-03, -4.8832e-02,\n",
       "         -1.9547e-01,  3.2408e-01,  3.4159e-02,  3.3183e-02, -2.9435e-01,\n",
       "          9.2712e-02,  1.5766e-01, -2.0469e-01,  3.9426e-01,  8.8201e-03,\n",
       "          4.4532e-01, -5.6977e-01,  9.7586e-02, -9.2824e-02,  8.9061e-02],\n",
       "        [-1.3005e-01, -1.5919e-01, -1.7304e-02,  6.0464e-02, -1.8195e-02,\n",
       "         -1.6630e-02,  5.2267e-01,  4.5211e-01, -6.2347e-02, -4.9695e-01,\n",
       "          4.6547e-03,  1.2648e-01, -1.0728e-01,  2.5645e-01,  1.8271e-01,\n",
       "          2.9843e-01, -3.1952e-01,  2.9193e-01,  4.8406e-01,  3.0013e-01],\n",
       "        [-1.4873e-01, -1.3735e-01,  1.3134e-02,  7.8055e-02, -3.6552e-02,\n",
       "         -1.2494e-01,  3.2979e-01,  3.7040e-01,  9.5707e-02, -4.8978e-01,\n",
       "          1.8366e-01,  1.4320e-01, -1.3575e-01,  3.4945e-01, -1.9263e-02,\n",
       "          3.0416e-01, -4.5762e-01,  1.3722e-02, -1.8605e-02,  8.4121e-02],\n",
       "        [-3.4781e-02, -4.8583e-02, -8.7236e-02,  5.0827e-02, -4.6217e-02,\n",
       "         -1.9458e-01,  4.1971e-01,  1.9768e-01, -1.6731e-01, -3.2742e-01,\n",
       "          5.8226e-02,  2.6438e-01, -1.8376e-01,  3.7410e-01,  3.9728e-02,\n",
       "          5.7683e-01, -5.5705e-01,  8.9737e-02, -2.7517e-01,  1.5714e-01],\n",
       "        [-2.5376e-01, -6.2503e-02, -2.4546e-01, -8.1797e-02, -4.8565e-02,\n",
       "         -6.9636e-02,  5.0345e-01,  5.2844e-01, -3.7935e-01, -4.3343e-01,\n",
       "          1.2119e-01,  2.6953e-01, -8.0690e-03,  1.8620e-01,  1.3315e-01,\n",
       "          2.4986e-01, -1.8526e-01,  3.4670e-01,  2.8030e-01,  1.4684e-01],\n",
       "        [-2.0011e-01, -3.3648e-01, -1.3767e-01,  1.6819e-02, -1.0293e-01,\n",
       "          6.6591e-02,  5.0165e-01,  1.4457e-01, -1.8910e-02, -3.4178e-01,\n",
       "          2.6759e-01,  2.1020e-01, -4.3501e-02,  4.0996e-01,  7.2426e-02,\n",
       "          2.1387e-01, -5.5092e-01,  2.4807e-01, -1.8747e-01, -8.9430e-02],\n",
       "        [-2.5769e-01, -2.4881e-01, -1.8819e-01,  2.0377e-01,  2.9628e-02,\n",
       "         -1.5824e-01,  3.4656e-01,  2.6684e-01, -2.9486e-01, -4.1710e-01,\n",
       "          1.8423e-01,  5.3881e-02,  1.3272e-02,  3.4793e-01,  6.6106e-02,\n",
       "          3.4923e-01, -4.6474e-01,  3.2872e-01, -1.8412e-01,  4.2469e-02],\n",
       "        [-1.9681e-01, -1.9141e-01, -3.7130e-02,  1.2727e-01, -1.1820e-01,\n",
       "         -2.1086e-01,  4.3037e-01,  1.3808e-01, -1.5899e-01, -4.5741e-01,\n",
       "          1.5009e-01,  3.9597e-01, -2.2446e-01,  4.0848e-01, -2.4464e-02,\n",
       "          2.6467e-01, -4.6321e-01, -5.1074e-02, -2.2007e-01,  2.2583e-01],\n",
       "        [-1.9418e-01, -8.1099e-02,  1.3361e-01,  4.9621e-02, -8.0864e-03,\n",
       "         -3.6174e-01,  5.8511e-01,  8.5643e-02, -3.2211e-02, -2.7764e-01,\n",
       "          4.2588e-01,  2.0564e-01, -6.2544e-02,  4.2934e-01, -2.6380e-02,\n",
       "          5.1139e-01, -5.6442e-01,  9.0325e-02, -2.6542e-01,  7.8114e-02],\n",
       "        [-1.1008e-01, -2.3889e-01, -4.0762e-02,  6.2155e-02, -3.0530e-02,\n",
       "         -1.9472e-01,  3.5016e-01,  2.8193e-01, -1.6877e-01, -6.1440e-01,\n",
       "          1.9119e-01,  2.3450e-01, -2.3332e-01,  3.5083e-01,  8.3346e-02,\n",
       "          4.4078e-01, -5.9056e-01,  8.8862e-02, -2.1914e-01,  1.4515e-01],\n",
       "        [-1.5693e-01, -1.3208e-01, -2.5522e-02,  1.9404e-01, -1.2922e-01,\n",
       "         -1.0635e-01,  5.1014e-01,  1.7043e-01, -1.8802e-01, -6.3117e-01,\n",
       "          1.6538e-01,  1.4041e-01, -3.2815e-01,  4.4400e-01, -1.2883e-03,\n",
       "          1.9099e-01, -7.6459e-01,  2.1571e-01,  4.4547e-02,  1.9916e-01],\n",
       "        [-3.3285e-02, -3.0078e-02,  7.2243e-02, -1.7868e-01, -1.8790e-02,\n",
       "         -5.8134e-03,  3.2039e-01,  4.4220e-01, -1.8585e-01, -2.9477e-01,\n",
       "          2.9281e-01, -2.3359e-02,  2.0373e-01,  8.5765e-02,  1.2643e-01,\n",
       "          4.3934e-01, -2.9895e-01,  1.7422e-01,  5.7071e-01,  3.4610e-01],\n",
       "        [-2.8086e-01,  1.6367e-02, -1.0123e-01,  3.5948e-01, -8.7963e-02,\n",
       "         -1.6652e-01,  2.5529e-01,  1.6553e-01, -1.8780e-01, -3.0856e-01,\n",
       "          2.2072e-01,  1.8236e-01, -9.6714e-02,  4.8286e-01,  1.7038e-01,\n",
       "          1.8827e-01, -4.9546e-01,  2.7842e-01, -3.1966e-01,  1.9826e-01],\n",
       "        [-8.5667e-03, -2.0721e-01, -6.9187e-02,  1.2632e-01, -1.3313e-01,\n",
       "         -1.2344e-01,  3.4958e-01,  1.7889e-01, -4.5777e-02, -2.6897e-01,\n",
       "          2.4338e-01,  2.1103e-01, -1.2267e-01,  3.5639e-01, -5.4928e-02,\n",
       "          1.4846e-01, -6.7515e-01,  9.2450e-02, -1.3054e-01,  1.4237e-01],\n",
       "        [-1.6529e-01, -1.6853e-01, -1.5957e-01, -1.0763e-01, -1.9058e-01,\n",
       "         -1.5760e-01,  4.8480e-01,  2.4681e-01, -4.0817e-02, -3.1466e-01,\n",
       "          1.1572e-01,  2.1489e-01,  4.8594e-02,  3.4366e-01,  7.8820e-02,\n",
       "          3.9749e-01, -5.1211e-01,  4.1158e-01,  7.3069e-02,  5.4440e-02],\n",
       "        [-1.5784e-01, -2.0871e-01, -2.5738e-02,  8.5526e-02, -1.9760e-01,\n",
       "         -2.5262e-01,  3.1885e-01,  7.2812e-02,  5.3330e-02, -4.5453e-01,\n",
       "          1.4328e-01,  2.3372e-01, -4.4695e-02,  4.9629e-01,  1.5062e-01,\n",
       "          3.1486e-01, -5.0039e-01,  1.3823e-01, -8.4736e-02,  2.3079e-02],\n",
       "        [-2.5923e-01, -3.7239e-01,  1.4228e-01,  1.9776e-01, -2.5221e-01,\n",
       "         -8.7708e-02,  3.3088e-01,  8.8807e-02, -1.9406e-01, -2.4143e-01,\n",
       "          2.3453e-01,  3.4238e-01, -1.5981e-01,  2.0950e-01,  5.4463e-02,\n",
       "          3.0826e-01, -4.5487e-01,  1.8940e-01, -1.2071e-01,  1.7330e-01],\n",
       "        [-1.9810e-01, -1.5188e-01,  1.3235e-02,  3.1520e-02, -1.3012e-01,\n",
       "         -7.7287e-02,  4.1285e-01, -1.0248e-02, -3.5457e-01, -4.1115e-01,\n",
       "          1.8654e-01,  9.0277e-02, -9.8340e-02,  3.7998e-01,  2.3462e-01,\n",
       "          4.6717e-01, -4.1217e-01, -5.7154e-02, -1.9265e-01, -3.1524e-02],\n",
       "        [-1.0640e-01, -4.6982e-02,  8.0088e-02,  5.8800e-02, -9.8701e-02,\n",
       "         -3.6913e-01,  3.7291e-01,  1.3353e-01,  3.3288e-04, -3.5105e-01,\n",
       "          2.3450e-01,  1.2310e-01, -1.2381e-01,  3.5515e-01, -5.3217e-02,\n",
       "          4.9753e-01, -6.2394e-01, -1.6786e-02, -1.5224e-01,  1.6627e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 5875, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2043, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 3728, 1010,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2026, 6685,  ...,    0,    0,    0],\n",
      "        [ 101, 1034, 1034,  ...,    0,    0,    0],\n",
      "        [ 101, 1024, 2009,  ...,    0,    0,    0]])\n",
      "tensor([ 8,  4, 12, 15,  8,  1, 15, 16, 12, 12, 16, 19,  6, 17,  1, 17, 17,  2,\n",
      "         9,  5,  9, 12, 19,  4,  2,  6,  3,  6,  9,  0,  8,  9])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 4],\n",
       "        [5, 2],\n",
       "        [6, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCLDL",
   "language": "python",
   "name": "cscldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

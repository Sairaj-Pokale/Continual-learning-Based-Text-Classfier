{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\CSCLDL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Union, Tuple, List, Iterable, Dict\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "#from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import gzip, csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.init as init\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def scaled_dot_product(q, k, v, attn_drop_rate=0.1, mask=None):\n",
    "def scaled_dot_product(q, k, v, attn_drop_rate=0.1):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      q: query, shape: (batch, # heads, seq len, head dimension)\n",
    "      k: keys, shape: (batch, # heads, seq len, head dimension)\n",
    "      v: value, shape: (batch, # heads, seq len, head dimension)\n",
    "      attn_drop_rate: probability of an element to be zeroed,\n",
    "      mask: the optional masking of specific entries in the attention matrix.\n",
    "              shape: (batch, seq len)\n",
    "    \"\"\"\n",
    "    \n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-1, -2))\n",
    "    attn_logits = attn_logits/math.sqrt(d_k)\n",
    "    # if mask is not None:\n",
    "    #   dummy_mask = torch.where(mask == 1.0, torch.tensor(True), torch.tensor(False))\n",
    "    #   attn_logits = attn_logits.masked_fill(dummy_mask.unsqueeze(1).unsqueeze(1), float('-inf'))\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    attention = F.dropout(attention)\n",
    "    values = torch.matmul(attention,v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.query = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "      nn.init.xavier_uniform_(self.query.weight)\n",
    "      self.query.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.key.weight)\n",
    "      self.key.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.value.weight)\n",
    "      self.value.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "      self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def split_heads(self, tensor):\n",
    "       new_shape = tensor.size()[:-1] + (self.n_heads, self.head_dim)\n",
    "       tensor = tensor.view(*new_shape)\n",
    "       tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "       return tensor\n",
    "    \n",
    "    def merge_heads(self, tensor, batch_size, seq_length):\n",
    "       tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "       return tensor\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "       batch_size, seq_length, embed_dim = embedding.size()\n",
    "       q, k, v = self.query(embedding), self.key(embedding), self.value(embedding)\n",
    "       q = self.split_heads(q)\n",
    "       k = self.split_heads(k)\n",
    "       v = self.split_heads(v)\n",
    "       values = scaled_dot_product(q, k, v, self.attn_drop_rate)\n",
    "       values = self.merge_heads(values, batch_size, seq_length)\n",
    "       attended_embeds = self.o_proj(values)\n",
    "\n",
    "       return attended_embeds\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        print(f\"Mean ({mean.size()})\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation  ({std.size()})\")\n",
    "        y = (inputs - mean) / std\n",
    "        print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y  + self.beta\n",
    "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
    "        print(f\"out: {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, 4*embed_dim)\n",
    "        self.linear2 = nn.Linear(4*embed_dim, embed_dim)\n",
    "        self.relu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier: #mewmew\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, numclasses, dropout_rate=0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(input_dim, numclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads, attn_drop_rate)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout1 = nn.Dropout(p=layer_drop_rate)\n",
    "        self.ffn = PositionwiseFeedForward(self.embed_dim,layer_drop_rate)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout2 = nn.Dropout(p=layer_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_x = x\n",
    "        print(\"------- ATTENTION 1 ------\")\n",
    "        x = self.attention(x)\n",
    "        print(\"------- DROPOUT 1 ------\")\n",
    "        x = self.dropout1(x)\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
    "        x = x + residual_x\n",
    "        x = self.norm1(x)\n",
    "        residual_x = x\n",
    "        print(\"------- ATTENTION 2 ------\")\n",
    "        x = self.ffn(x)\n",
    "        print(\"------- DROPOUT 2 ------\")\n",
    "        x = self.dropout2(x)\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
    "        x = x + residual_x\n",
    "        x = self.norm2(x)\n",
    "        #add and norm switch refer to assignment\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([3, 2, 64])\n",
      "x after 2nd linear layer: torch.Size([3, 2, 16])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "Output shape:  (3, 2, 16)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16\n",
    "n_heads = 4\n",
    "attn_drop_rate = 0.1\n",
    "layer_drop_rate = 0.1\n",
    "block = EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n",
    "\n",
    "bs = 3\n",
    "seq_len = 2\n",
    "embeds = torch.randn(bs, seq_len, embed_dim)\n",
    "outputs = block(embeds)\n",
    "out_bs, out_seq_len, out_hidden = outputs.shape\n",
    "print(\"Output shape: \", (out_bs, out_seq_len, out_hidden))\n",
    "assert out_bs == bs and out_seq_len == seq_len and out_hidden == embed_dim, \"Unexpected output shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module): #Transformer\n",
    "    def __init__(self, n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+1, embed_dim)\n",
    "        self.position = PositionalEncoding(embed_dim, layer_drop_rate)\n",
    "        self.net = nn.Sequential(*[\n",
    "        EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.mask_pred = nn.Linear(embed_dim, vocab_size) ## Classifier\n",
    "        self.classifier = Classifier(embed_dim, num_classes)\n",
    "    def forward(self, batch_text):\n",
    "        embedding = self.position(self.embed(batch_text))\n",
    "        new_embedding = self.net((embedding))\n",
    "        mask_preds = self.mask_pred(new_embedding)\n",
    "        return mask_preds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([3, 2, 64])\n",
      "x after 2nd linear layer: torch.Size([3, 2, 16])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "------- ATTENTION 2 ------\n",
      "x after dropout: torch.Size([3, 2, 64])\n",
      "x after 2nd linear layer: torch.Size([3, 2, 16])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([3, 2, 1]))\n",
      "Standard Deviation  (torch.Size([3, 2, 1]))\n",
      "y: torch.Size([3, 2, 16])\n",
      "self.gamma: torch.Size([16]), self.beta: torch.Size([16])\n",
      "out: torch.Size([3, 2, 16])\n",
      "Mask predictions shape:  (3, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "vocab_size = 10\n",
    "attn_drop_rate = 0.1\n",
    "layer_drop_rate = 0.1\n",
    "num_classes=20\n",
    "model = BERT(n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate)\n",
    "\n",
    "# bs = 3\n",
    "# seq_len = 2\n",
    "# inputs = torch.randint(0, vocab_size, (bs, seq_len))\n",
    "# mask_preds = model(inputs)\n",
    "# out_bs, out_seq_len, out_vocab = mask_preds.shape\n",
    "# print(\"Mask predictions shape: \", (out_bs, out_seq_len, out_vocab))\n",
    "# assert out_bs == bs and out_seq_len == seq_len and out_vocab == vocab_size, \"Unexpected mask prediction output shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Replace with your desired tokenizer\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.tokenizer(item['text'], truncation=True, padding='max_length', return_tensors='pt', max_length=512)\n",
    "        encoding['label'] = torch.tensor(item['label'])\n",
    "        return encoding\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"setfit/20_newsgroups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 11314\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 7532\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"setfit/20_newsgroups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  2001,  6603,  2065,  3087,  2041,  2045,  2071,  4372,\n",
       "          7138,  2368,  2033,  2006,  2023,  2482,  1045,  2387,  1996,  2060,\n",
       "          2154,  1012,  2009,  2001,  1037,  1016,  1011,  2341,  2998,  2482,\n",
       "          1010,  2246,  2000,  2022,  2013,  1996,  2397, 20341,  1013,  2220,\n",
       "         17549,  1012,  2009,  2001,  2170,  1037,  5318,  4115,  1012,  1996,\n",
       "          4303,  2020,  2428,  2235,  1012,  1999,  2804,  1010,  1996,  2392,\n",
       "         21519,  2001,  3584,  2013,  1996,  2717,  1997,  1996,  2303,  1012,\n",
       "          2023,  2003,  2035,  1045,  2113,  1012,  2065,  3087,  2064,  2425,\n",
       "          4168,  1037,  2944,  2171,  1010,  3194, 28699,  2015,  1010,  2086,\n",
       "          1997,  2537,  1010,  2073,  2023,  2482,  2003,  2081,  1010,  2381,\n",
       "          1010,  2030,  3649, 18558,  2017,  2031,  2006,  2023, 24151,  2559,\n",
       "          2482,  1010,  3531,  1041,  1011,  5653,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCLDL",
   "language": "python",
   "name": "cscldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

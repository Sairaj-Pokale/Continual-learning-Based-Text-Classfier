{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7m5iPfVANhfa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from typing import Union, Tuple, List, Iterable, Dict\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import numpy as np\n",
        "import gzip, csv\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45kZv80K968l"
      },
      "outputs": [],
      "source": [
        "! pip3 install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pPDJEnvNsjR"
      },
      "outputs": [],
      "source": [
        "# %pip install transformers\n",
        "# from transformers import AutoTokenizer\n",
        "# # If you can not find all the bugs, use the line below for AutoModel\n",
        "# #from transformers import AutoModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbyQ_X_a7ma3"
      },
      "outputs": [],
      "source": [
        "! pip3 install avalanche-lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8ObZdvN8F5w"
      },
      "outputs": [],
      "source": [
        "! pip3 install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xwliFn7kPeBH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class Config(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 hidden_size=512,\n",
        "                 num_hidden_layers=6,\n",
        "                 num_attention_heads=8,\n",
        "                 intermediate_size=2048,\n",
        "                 dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02):\n",
        "        \"\"\"Constructs Config for BertModel.\"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = dropout_prob\n",
        "        self.attention_probs_dropout_prob = dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dict_object):\n",
        "        \"\"\"Constructs Config from a Python dictionary of parameters.\"\"\"\n",
        "        config = Config(vocab_size=None)\n",
        "        for (key, value) in dict_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization module.\"\"\"\n",
        "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
        "        \"\"\"Constructs LayerNorm object for Transformer layer in BERT model.\"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the LayerNorm layer.\"\"\"\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Feed forward network with gelu activation.\"\"\"\n",
        "    def __init__(self, hidden_size, intermediate_size):\n",
        "        \"\"\"Constructs MLP object for Transformer layer in BERT model.\"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.dense_expansion = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.dense_contraction = nn.Linear(intermediate_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the MLP layer.\"\"\"\n",
        "        x = self.dense_expansion(x)\n",
        "        x = self.dense_contraction(gelu(x))\n",
        "        return x\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    \"\"\"The Transformer layer.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Constructs Layer object for Transformer layer in BERT model based on config.\"\"\"\n",
        "        super(Layer, self).__init__()\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "        self.attn_out = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.ln1 = LayerNorm(config.hidden_size)\n",
        "\n",
        "        self.mlp = MLP(config.hidden_size, config.intermediate_size)\n",
        "        self.ln2 = LayerNorm(config.hidden_size)\n",
        "\n",
        "    def split_heads(self, tensor, num_heads, attention_head_size):\n",
        "        \"\"\"Split hidden_size into num_heads * attention_head_size and transpose into shape [batch, num_heads, seq_len, attention_head_size].\"\"\"\n",
        "        new_shape = tensor.size()[:-1] + (num_heads, attention_head_size)\n",
        "        tensor = tensor.view(*new_shape)\n",
        "        return tensor.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    def merge_heads(self, tensor, num_heads, attention_head_size):\n",
        "        \"\"\"Transpose and then reshape into shape [batch, seq_len, hidden_size].\"\"\"\n",
        "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = tensor.size()[:-2] + (num_heads * attention_head_size,)\n",
        "        return tensor.view(new_shape)\n",
        "\n",
        "    def attn(self, q, k, v, attention_mask):\n",
        "        \"\"\"Attention mechanism for the Transformer layer.\"\"\"\n",
        "        mask = attention_mask == 1\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        s = torch.matmul(q, k.transpose(-1, -2))\n",
        "        s = s / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        s = torch.where(mask, s, torch.tensor(float('-inf')))\n",
        "\n",
        "        p = F.softmax(s, dim=-1)\n",
        "        p = self.dropout(p)\n",
        "\n",
        "        a = torch.matmul(p, v)\n",
        "        return a\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        \"\"\"Forward pass of the Transformer layer in BERT.\"\"\"\n",
        "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
        "\n",
        "        q = self.split_heads(q, self.num_attention_heads, self.attention_head_size)\n",
        "        k = self.split_heads(k, self.num_attention_heads, self.attention_head_size)\n",
        "        v = self.split_heads(v, self.num_attention_heads, self.attention_head_size)\n",
        "\n",
        "        a = self.attn(q, k, v, attention_mask)\n",
        "        a = self.merge_heads(a, self.num_attention_heads, self.attention_head_size)\n",
        "        a = self.attn_out(a)\n",
        "        a = self.dropout(a)\n",
        "        a = self.ln1(a + x)\n",
        "\n",
        "        m = self.mlp(a)\n",
        "        m = self.dropout(m)\n",
        "        m = self.ln2(m + a)\n",
        "\n",
        "        return m\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, config_dict):\n",
        "        super(Bert, self).__init__()\n",
        "        self.config = Config.from_dict(config_dict)  # Create an instance of Config\n",
        "\n",
        "        self.embeddings = nn.ModuleDict({\n",
        "            'token': nn.Embedding(self.config.vocab_size, self.config.hidden_size, padding_idx=0),\n",
        "            'position': nn.Embedding(self.config.max_position_embeddings, self.config.hidden_size),\n",
        "            'token_type': nn.Embedding(self.config.type_vocab_size, self.config.hidden_size),\n",
        "        })\n",
        "\n",
        "        self.ln = LayerNorm(self.config.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            Layer(self.config) for _ in range(self.config.num_hidden_layers)\n",
        "        ])\n",
        "\n",
        "        self.pooler = nn.Sequential(OrderedDict([\n",
        "            ('dense', nn.Linear(self.config.hidden_size, self.config.hidden_size)),\n",
        "            ('activation', nn.Tanh()),\n",
        "        ]))\n",
        "\n",
        "        # Add a classifier layer for classification\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, 20)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        x = (self.embeddings['token'](input_ids) +\n",
        "             self.embeddings['position'](position_ids) +\n",
        "             self.embeddings['token_type'](token_type_ids))\n",
        "        x = self.dropout(self.ln(x))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "\n",
        "        o = self.pooler(x[:, 0])\n",
        "\n",
        "        if labels is not None:\n",
        "          # Use the classifier layer for classification\n",
        "          logits = F.softmax(self.classifier(o), dim=1)\n",
        "          #loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "          return logits\n",
        "\n",
        "        return x, o\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save model to a file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the file where the model will be saved.\n",
        "        \"\"\"\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, config_dict, path):\n",
        "        \"\"\"Load model from a file.\n",
        "\n",
        "        Args:\n",
        "            config_dict (dict): Dictionary containing the configuration of the model.\n",
        "            path (str): Path to the model checkpoint.\n",
        "\n",
        "        Returns:\n",
        "            Bert: Bert model loaded from the checkpoint.\n",
        "        \"\"\"\n",
        "        model = cls(config_dict)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LVi3aTlg1HxT",
        "outputId": "e39a090e-4e9a-47ab-988c-cdd0dabf53cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\saira\\anaconda3\\envs\\CSCLDL\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
            "c:\\Users\\saira\\anaconda3\\envs\\CSCLDL\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Batch 10/354 - Avg Loss: 3.0052\n",
            "Batch 20/354 - Avg Loss: 3.0037\n",
            "Batch 30/354 - Avg Loss: 3.0030\n",
            "Batch 40/354 - Avg Loss: 3.0059\n",
            "Batch 50/354 - Avg Loss: 3.0052\n",
            "Batch 60/354 - Avg Loss: 3.0059\n",
            "Batch 70/354 - Avg Loss: 3.0043\n",
            "Batch 80/354 - Avg Loss: 3.0046\n",
            "Batch 90/354 - Avg Loss: 3.0043\n",
            "Batch 100/354 - Avg Loss: 3.0044\n",
            "Batch 110/354 - Avg Loss: 3.0042\n",
            "Batch 120/354 - Avg Loss: 3.0035\n",
            "Batch 130/354 - Avg Loss: 3.0038\n",
            "Batch 140/354 - Avg Loss: 3.0043\n",
            "Batch 150/354 - Avg Loss: 3.0061\n",
            "Batch 160/354 - Avg Loss: 3.0060\n",
            "Batch 170/354 - Avg Loss: 3.0064\n",
            "Batch 180/354 - Avg Loss: 3.0054\n",
            "Batch 190/354 - Avg Loss: 3.0055\n",
            "Batch 200/354 - Avg Loss: 3.0062\n",
            "Batch 210/354 - Avg Loss: 3.0062\n",
            "Batch 220/354 - Avg Loss: 3.0062\n",
            "Batch 230/354 - Avg Loss: 3.0075\n",
            "Batch 240/354 - Avg Loss: 3.0080\n",
            "Batch 250/354 - Avg Loss: 3.0081\n",
            "Batch 260/354 - Avg Loss: 3.0082\n",
            "Batch 270/354 - Avg Loss: 3.0082\n",
            "Batch 280/354 - Avg Loss: 3.0083\n",
            "Batch 290/354 - Avg Loss: 3.0084\n",
            "Batch 300/354 - Avg Loss: 3.0079\n",
            "Batch 310/354 - Avg Loss: 3.0081\n",
            "Batch 320/354 - Avg Loss: 3.0087\n",
            "Batch 330/354 - Avg Loss: 3.0094\n",
            "Batch 340/354 - Avg Loss: 3.0095\n",
            "Batch 350/354 - Avg Loss: 3.0103\n",
            "Epoch 1 - Avg Loss: 3.0106\n",
            "Epoch 2/10\n",
            "Batch 10/354 - Avg Loss: 3.0531\n",
            "Batch 20/354 - Avg Loss: 3.0391\n",
            "Batch 30/354 - Avg Loss: 3.0427\n",
            "Batch 40/354 - Avg Loss: 3.0422\n",
            "Batch 50/354 - Avg Loss: 3.0431\n",
            "Batch 60/354 - Avg Loss: 3.0386\n",
            "Batch 70/354 - Avg Loss: 3.0384\n",
            "Batch 80/354 - Avg Loss: 3.0348\n",
            "Batch 90/354 - Avg Loss: 3.0337\n",
            "Batch 100/354 - Avg Loss: 3.0335\n",
            "Batch 110/354 - Avg Loss: 3.0316\n",
            "Batch 120/354 - Avg Loss: 3.0315\n",
            "Batch 130/354 - Avg Loss: 3.0303\n",
            "Batch 140/354 - Avg Loss: 3.0299\n",
            "Batch 150/354 - Avg Loss: 3.0302\n",
            "Batch 160/354 - Avg Loss: 3.0287\n",
            "Batch 170/354 - Avg Loss: 3.0280\n",
            "Batch 180/354 - Avg Loss: 3.0280\n",
            "Batch 190/354 - Avg Loss: 3.0270\n",
            "Batch 200/354 - Avg Loss: 3.0272\n",
            "Batch 210/354 - Avg Loss: 3.0273\n",
            "Batch 220/354 - Avg Loss: 3.0279\n",
            "Batch 230/354 - Avg Loss: 3.0277\n",
            "Batch 240/354 - Avg Loss: 3.0268\n",
            "Batch 250/354 - Avg Loss: 3.0260\n",
            "Batch 260/354 - Avg Loss: 3.0256\n",
            "Batch 270/354 - Avg Loss: 3.0244\n",
            "Batch 280/354 - Avg Loss: 3.0245\n",
            "Batch 290/354 - Avg Loss: 3.0248\n",
            "Batch 300/354 - Avg Loss: 3.0250\n",
            "Batch 310/354 - Avg Loss: 3.0255\n",
            "Batch 320/354 - Avg Loss: 3.0246\n",
            "Batch 330/354 - Avg Loss: 3.0246\n",
            "Batch 340/354 - Avg Loss: 3.0248\n",
            "Batch 350/354 - Avg Loss: 3.0250\n",
            "Epoch 2 - Avg Loss: 3.0254\n",
            "Epoch 3/10\n",
            "Batch 10/354 - Avg Loss: 3.0563\n",
            "Batch 20/354 - Avg Loss: 3.0360\n",
            "Batch 30/354 - Avg Loss: 3.0281\n",
            "Batch 40/354 - Avg Loss: 3.0297\n",
            "Batch 50/354 - Avg Loss: 3.0300\n",
            "Batch 60/354 - Avg Loss: 3.0334\n",
            "Batch 70/354 - Avg Loss: 3.0322\n",
            "Batch 80/354 - Avg Loss: 3.0297\n",
            "Batch 90/354 - Avg Loss: 3.0278\n",
            "Batch 100/354 - Avg Loss: 3.0285\n",
            "Batch 110/354 - Avg Loss: 3.0262\n",
            "Batch 120/354 - Avg Loss: 3.0250\n",
            "Batch 130/354 - Avg Loss: 3.0253\n",
            "Batch 140/354 - Avg Loss: 3.0255\n",
            "Batch 150/354 - Avg Loss: 3.0240\n",
            "Batch 160/354 - Avg Loss: 3.0256\n",
            "Batch 170/354 - Avg Loss: 3.0259\n",
            "Batch 180/354 - Avg Loss: 3.0261\n",
            "Batch 190/354 - Avg Loss: 3.0272\n",
            "Batch 200/354 - Avg Loss: 3.0269\n",
            "Batch 210/354 - Avg Loss: 3.0261\n",
            "Batch 220/354 - Avg Loss: 3.0270\n",
            "Batch 230/354 - Avg Loss: 3.0262\n",
            "Batch 240/354 - Avg Loss: 3.0268\n",
            "Batch 250/354 - Avg Loss: 3.0265\n",
            "Batch 260/354 - Avg Loss: 3.0251\n",
            "Batch 270/354 - Avg Loss: 3.0247\n",
            "Batch 280/354 - Avg Loss: 3.0251\n",
            "Batch 290/354 - Avg Loss: 3.0253\n",
            "Batch 300/354 - Avg Loss: 3.0248\n",
            "Batch 310/354 - Avg Loss: 3.0249\n",
            "Batch 320/354 - Avg Loss: 3.0255\n",
            "Batch 330/354 - Avg Loss: 3.0252\n",
            "Batch 340/354 - Avg Loss: 3.0252\n",
            "Batch 350/354 - Avg Loss: 3.0254\n",
            "Epoch 3 - Avg Loss: 3.0254\n",
            "Epoch 4/10\n",
            "Batch 10/354 - Avg Loss: 3.0219\n",
            "Batch 20/354 - Avg Loss: 3.0266\n",
            "Batch 30/354 - Avg Loss: 3.0334\n",
            "Batch 40/354 - Avg Loss: 3.0305\n",
            "Batch 50/354 - Avg Loss: 3.0269\n",
            "Batch 60/354 - Avg Loss: 3.0266\n",
            "Batch 70/354 - Avg Loss: 3.0241\n",
            "Batch 80/354 - Avg Loss: 3.0266\n",
            "Batch 90/354 - Avg Loss: 3.0271\n",
            "Batch 100/354 - Avg Loss: 3.0266\n",
            "Batch 110/354 - Avg Loss: 3.0264\n",
            "Batch 120/354 - Avg Loss: 3.0258\n",
            "Batch 130/354 - Avg Loss: 3.0255\n",
            "Batch 140/354 - Avg Loss: 3.0264\n",
            "Batch 150/354 - Avg Loss: 3.0265\n",
            "Batch 160/354 - Avg Loss: 3.0258\n",
            "Batch 170/354 - Avg Loss: 3.0259\n",
            "Batch 180/354 - Avg Loss: 3.0262\n",
            "Batch 190/354 - Avg Loss: 3.0267\n",
            "Batch 200/354 - Avg Loss: 3.0272\n",
            "Batch 210/354 - Avg Loss: 3.0280\n",
            "Batch 220/354 - Avg Loss: 3.0276\n",
            "Batch 230/354 - Avg Loss: 3.0267\n",
            "Batch 240/354 - Avg Loss: 3.0262\n",
            "Batch 250/354 - Avg Loss: 3.0253\n",
            "Batch 260/354 - Avg Loss: 3.0253\n",
            "Batch 270/354 - Avg Loss: 3.0256\n",
            "Batch 280/354 - Avg Loss: 3.0255\n",
            "Batch 290/354 - Avg Loss: 3.0255\n",
            "Batch 300/354 - Avg Loss: 3.0251\n",
            "Batch 310/354 - Avg Loss: 3.0247\n",
            "Batch 320/354 - Avg Loss: 3.0247\n",
            "Batch 330/354 - Avg Loss: 3.0248\n",
            "Batch 340/354 - Avg Loss: 3.0250\n",
            "Batch 350/354 - Avg Loss: 3.0254\n",
            "Epoch 4 - Avg Loss: 3.0254\n",
            "Epoch 5/10\n",
            "Batch 10/354 - Avg Loss: 3.0188\n",
            "Batch 20/354 - Avg Loss: 3.0266\n",
            "Batch 30/354 - Avg Loss: 3.0250\n",
            "Batch 40/354 - Avg Loss: 3.0281\n",
            "Batch 50/354 - Avg Loss: 3.0256\n",
            "Batch 60/354 - Avg Loss: 3.0276\n",
            "Batch 70/354 - Avg Loss: 3.0268\n",
            "Batch 80/354 - Avg Loss: 3.0285\n",
            "Batch 90/354 - Avg Loss: 3.0278\n",
            "Batch 100/354 - Avg Loss: 3.0266\n",
            "Batch 110/354 - Avg Loss: 3.0253\n",
            "Batch 120/354 - Avg Loss: 3.0242\n",
            "Batch 130/354 - Avg Loss: 3.0250\n",
            "Batch 140/354 - Avg Loss: 3.0257\n",
            "Batch 150/354 - Avg Loss: 3.0254\n",
            "Batch 160/354 - Avg Loss: 3.0254\n",
            "Batch 170/354 - Avg Loss: 3.0247\n",
            "Batch 180/354 - Avg Loss: 3.0236\n",
            "Batch 190/354 - Avg Loss: 3.0240\n",
            "Batch 200/354 - Avg Loss: 3.0253\n",
            "Batch 210/354 - Avg Loss: 3.0261\n",
            "Batch 220/354 - Avg Loss: 3.0263\n",
            "Batch 230/354 - Avg Loss: 3.0261\n",
            "Batch 240/354 - Avg Loss: 3.0272\n",
            "Batch 250/354 - Avg Loss: 3.0270\n",
            "Batch 260/354 - Avg Loss: 3.0267\n",
            "Batch 270/354 - Avg Loss: 3.0269\n",
            "Batch 280/354 - Avg Loss: 3.0266\n",
            "Batch 290/354 - Avg Loss: 3.0261\n",
            "Batch 300/354 - Avg Loss: 3.0258\n",
            "Batch 310/354 - Avg Loss: 3.0256\n",
            "Batch 320/354 - Avg Loss: 3.0253\n",
            "Batch 330/354 - Avg Loss: 3.0256\n",
            "Batch 340/354 - Avg Loss: 3.0256\n",
            "Batch 350/354 - Avg Loss: 3.0250\n",
            "Epoch 5 - Avg Loss: 3.0254\n",
            "Epoch 6/10\n",
            "Batch 10/354 - Avg Loss: 3.0125\n",
            "Batch 20/354 - Avg Loss: 3.0297\n",
            "Batch 30/354 - Avg Loss: 3.0292\n",
            "Batch 40/354 - Avg Loss: 3.0258\n",
            "Batch 50/354 - Avg Loss: 3.0213\n",
            "Batch 60/354 - Avg Loss: 3.0219\n",
            "Batch 70/354 - Avg Loss: 3.0237\n",
            "Batch 80/354 - Avg Loss: 3.0254\n",
            "Batch 90/354 - Avg Loss: 3.0261\n",
            "Batch 100/354 - Avg Loss: 3.0263\n",
            "Batch 110/354 - Avg Loss: 3.0264\n",
            "Batch 120/354 - Avg Loss: 3.0261\n",
            "Batch 130/354 - Avg Loss: 3.0262\n",
            "Batch 140/354 - Avg Loss: 3.0273\n",
            "Batch 150/354 - Avg Loss: 3.0277\n",
            "Batch 160/354 - Avg Loss: 3.0280\n",
            "Batch 170/354 - Avg Loss: 3.0283\n",
            "Batch 180/354 - Avg Loss: 3.0280\n",
            "Batch 190/354 - Avg Loss: 3.0272\n",
            "Batch 200/354 - Avg Loss: 3.0264\n",
            "Batch 210/354 - Avg Loss: 3.0271\n",
            "Batch 220/354 - Avg Loss: 3.0276\n",
            "Batch 230/354 - Avg Loss: 3.0273\n",
            "Batch 240/354 - Avg Loss: 3.0270\n",
            "Batch 250/354 - Avg Loss: 3.0271\n",
            "Batch 260/354 - Avg Loss: 3.0265\n",
            "Batch 270/354 - Avg Loss: 3.0268\n",
            "Batch 280/354 - Avg Loss: 3.0265\n",
            "Batch 290/354 - Avg Loss: 3.0259\n",
            "Batch 300/354 - Avg Loss: 3.0258\n",
            "Batch 310/354 - Avg Loss: 3.0253\n",
            "Batch 320/354 - Avg Loss: 3.0251\n",
            "Batch 330/354 - Avg Loss: 3.0253\n",
            "Batch 340/354 - Avg Loss: 3.0254\n",
            "Batch 350/354 - Avg Loss: 3.0254\n",
            "Epoch 6 - Avg Loss: 3.0254\n",
            "Epoch 7/10\n",
            "Batch 10/354 - Avg Loss: 3.0375\n",
            "Batch 20/354 - Avg Loss: 3.0344\n",
            "Batch 30/354 - Avg Loss: 3.0386\n",
            "Batch 40/354 - Avg Loss: 3.0406\n",
            "Batch 50/354 - Avg Loss: 3.0369\n",
            "Batch 60/354 - Avg Loss: 3.0334\n",
            "Batch 70/354 - Avg Loss: 3.0326\n",
            "Batch 80/354 - Avg Loss: 3.0324\n",
            "Batch 90/354 - Avg Loss: 3.0334\n",
            "Batch 100/354 - Avg Loss: 3.0328\n",
            "Batch 110/354 - Avg Loss: 3.0301\n",
            "Batch 120/354 - Avg Loss: 3.0292\n",
            "Batch 130/354 - Avg Loss: 3.0289\n",
            "Batch 140/354 - Avg Loss: 3.0277\n",
            "Batch 150/354 - Avg Loss: 3.0284\n",
            "Batch 160/354 - Avg Loss: 3.0260\n",
            "Batch 170/354 - Avg Loss: 3.0261\n",
            "Batch 180/354 - Avg Loss: 3.0264\n",
            "Batch 190/354 - Avg Loss: 3.0255\n",
            "Batch 200/354 - Avg Loss: 3.0256\n",
            "Batch 210/354 - Avg Loss: 3.0258\n",
            "Batch 220/354 - Avg Loss: 3.0252\n",
            "Batch 230/354 - Avg Loss: 3.0248\n",
            "Batch 240/354 - Avg Loss: 3.0240\n",
            "Batch 250/354 - Avg Loss: 3.0235\n",
            "Batch 260/354 - Avg Loss: 3.0243\n",
            "Batch 270/354 - Avg Loss: 3.0249\n",
            "Batch 280/354 - Avg Loss: 3.0240\n",
            "Batch 290/354 - Avg Loss: 3.0246\n",
            "Batch 300/354 - Avg Loss: 3.0246\n",
            "Batch 310/354 - Avg Loss: 3.0247\n",
            "Batch 320/354 - Avg Loss: 3.0248\n",
            "Batch 330/354 - Avg Loss: 3.0255\n",
            "Batch 340/354 - Avg Loss: 3.0257\n",
            "Batch 350/354 - Avg Loss: 3.0256\n",
            "Epoch 7 - Avg Loss: 3.0252\n",
            "Epoch 8/10\n",
            "Batch 10/354 - Avg Loss: 3.0250\n",
            "Batch 20/354 - Avg Loss: 3.0281\n",
            "Batch 30/354 - Avg Loss: 3.0229\n",
            "Batch 40/354 - Avg Loss: 3.0242\n",
            "Batch 50/354 - Avg Loss: 3.0281\n",
            "Batch 60/354 - Avg Loss: 3.0302\n",
            "Batch 70/354 - Avg Loss: 3.0299\n",
            "Batch 80/354 - Avg Loss: 3.0270\n",
            "Batch 90/354 - Avg Loss: 3.0236\n",
            "Batch 100/354 - Avg Loss: 3.0250\n",
            "Batch 110/354 - Avg Loss: 3.0222\n",
            "Batch 120/354 - Avg Loss: 3.0222\n",
            "Batch 130/354 - Avg Loss: 3.0229\n",
            "Batch 140/354 - Avg Loss: 3.0223\n",
            "Batch 150/354 - Avg Loss: 3.0221\n",
            "Batch 160/354 - Avg Loss: 3.0229\n",
            "Batch 170/354 - Avg Loss: 3.0225\n",
            "Batch 180/354 - Avg Loss: 3.0233\n",
            "Batch 190/354 - Avg Loss: 3.0234\n",
            "Batch 200/354 - Avg Loss: 3.0228\n",
            "Batch 210/354 - Avg Loss: 3.0229\n",
            "Batch 220/354 - Avg Loss: 3.0239\n",
            "Batch 230/354 - Avg Loss: 3.0237\n",
            "Batch 240/354 - Avg Loss: 3.0237\n",
            "Batch 250/354 - Avg Loss: 3.0245\n",
            "Batch 260/354 - Avg Loss: 3.0250\n",
            "Batch 270/354 - Avg Loss: 3.0253\n",
            "Batch 280/354 - Avg Loss: 3.0257\n",
            "Batch 290/354 - Avg Loss: 3.0255\n",
            "Batch 300/354 - Avg Loss: 3.0252\n",
            "Batch 310/354 - Avg Loss: 3.0243\n",
            "Batch 320/354 - Avg Loss: 3.0246\n",
            "Batch 330/354 - Avg Loss: 3.0249\n",
            "Batch 340/354 - Avg Loss: 3.0254\n",
            "Batch 350/354 - Avg Loss: 3.0254\n",
            "Epoch 8 - Avg Loss: 3.0254\n",
            "Epoch 9/10\n",
            "Batch 10/354 - Avg Loss: 3.0344\n",
            "Batch 20/354 - Avg Loss: 3.0219\n",
            "Batch 30/354 - Avg Loss: 3.0188\n",
            "Batch 40/354 - Avg Loss: 3.0219\n",
            "Batch 50/354 - Avg Loss: 3.0213\n",
            "Batch 60/354 - Avg Loss: 3.0219\n",
            "Batch 70/354 - Avg Loss: 3.0237\n",
            "Batch 80/354 - Avg Loss: 3.0250\n",
            "Batch 90/354 - Avg Loss: 3.0254\n",
            "Batch 100/354 - Avg Loss: 3.0281\n",
            "Batch 110/354 - Avg Loss: 3.0276\n",
            "Batch 120/354 - Avg Loss: 3.0276\n",
            "Batch 130/354 - Avg Loss: 3.0267\n",
            "Batch 140/354 - Avg Loss: 3.0257\n",
            "Batch 150/354 - Avg Loss: 3.0263\n",
            "Batch 160/354 - Avg Loss: 3.0258\n",
            "Batch 170/354 - Avg Loss: 3.0267\n",
            "Batch 180/354 - Avg Loss: 3.0264\n",
            "Batch 190/354 - Avg Loss: 3.0268\n",
            "Batch 200/354 - Avg Loss: 3.0269\n",
            "Batch 210/354 - Avg Loss: 3.0268\n",
            "Batch 220/354 - Avg Loss: 3.0259\n",
            "Batch 230/354 - Avg Loss: 3.0256\n",
            "Batch 240/354 - Avg Loss: 3.0249\n",
            "Batch 250/354 - Avg Loss: 3.0250\n",
            "Batch 260/354 - Avg Loss: 3.0249\n",
            "Batch 270/354 - Avg Loss: 3.0248\n",
            "Batch 280/354 - Avg Loss: 3.0247\n",
            "Batch 290/354 - Avg Loss: 3.0245\n",
            "Batch 300/354 - Avg Loss: 3.0240\n",
            "Batch 310/354 - Avg Loss: 3.0243\n",
            "Batch 320/354 - Avg Loss: 3.0249\n",
            "Batch 330/354 - Avg Loss: 3.0252\n",
            "Batch 340/354 - Avg Loss: 3.0251\n",
            "Batch 350/354 - Avg Loss: 3.0255\n",
            "Epoch 9 - Avg Loss: 3.0254\n",
            "Epoch 10/10\n",
            "Batch 10/354 - Avg Loss: 3.0313\n",
            "Batch 20/354 - Avg Loss: 3.0219\n",
            "Batch 30/354 - Avg Loss: 3.0125\n",
            "Batch 40/354 - Avg Loss: 3.0180\n",
            "Batch 50/354 - Avg Loss: 3.0194\n",
            "Batch 60/354 - Avg Loss: 3.0209\n",
            "Batch 70/354 - Avg Loss: 3.0250\n",
            "Batch 80/354 - Avg Loss: 3.0231\n",
            "Batch 90/354 - Avg Loss: 3.0240\n",
            "Batch 100/354 - Avg Loss: 3.0247\n",
            "Batch 110/354 - Avg Loss: 3.0245\n",
            "Batch 120/354 - Avg Loss: 3.0248\n",
            "Batch 130/354 - Avg Loss: 3.0257\n",
            "Batch 140/354 - Avg Loss: 3.0257\n",
            "Batch 150/354 - Avg Loss: 3.0252\n",
            "Batch 160/354 - Avg Loss: 3.0252\n",
            "Batch 170/354 - Avg Loss: 3.0261\n",
            "Batch 180/354 - Avg Loss: 3.0257\n",
            "Batch 190/354 - Avg Loss: 3.0265\n",
            "Batch 200/354 - Avg Loss: 3.0267\n",
            "Batch 210/354 - Avg Loss: 3.0262\n",
            "Batch 220/354 - Avg Loss: 3.0255\n",
            "Batch 230/354 - Avg Loss: 3.0258\n",
            "Batch 240/354 - Avg Loss: 3.0255\n",
            "Batch 250/354 - Avg Loss: 3.0248\n",
            "Batch 260/354 - Avg Loss: 3.0245\n",
            "Batch 270/354 - Avg Loss: 3.0242\n",
            "Batch 280/354 - Avg Loss: 3.0240\n",
            "Batch 290/354 - Avg Loss: 3.0243\n",
            "Batch 300/354 - Avg Loss: 3.0238\n",
            "Batch 310/354 - Avg Loss: 3.0243\n",
            "Batch 320/354 - Avg Loss: 3.0250\n",
            "Batch 330/354 - Avg Loss: 3.0250\n",
            "Batch 340/354 - Avg Loss: 3.0257\n",
            "Batch 350/354 - Avg Loss: 3.0251\n",
            "Epoch 10 - Avg Loss: 3.0254\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Define your custom BERT configuration\n",
        "config_dict = {\n",
        "    'vocab_size': 310000,\n",
        "    'hidden_size': 512,\n",
        "    'num_attention_heads': 2,\n",
        "    'num_hidden_layers': 4,\n",
        "    'intermediate_size': 512,\n",
        "    'dropout_prob': 0.1,\n",
        "    'max_position_embeddings': 512,\n",
        "    'type_vocab_size': 2,\n",
        "    'initializer_range': 0.02\n",
        "}\n",
        "\n",
        "# Create an instance of your custom BERT model\n",
        "model = Bert(config_dict)\n",
        "\n",
        "# Tokenizer for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Replace with your desired tokenizer\n",
        "\n",
        "# Define a DataLoader for the dataset\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.tokenizer(item['text'], truncation=True, padding='max_length', return_tensors='pt', max_length=512)\n",
        "        encoding['label'] = torch.tensor(item['label'])\n",
        "        return encoding\n",
        "\n",
        "# Use DynamicPaddingCollate for DataLoader\n",
        "class DynamicPaddingCollate:\n",
        "    def __call__(self, batch):\n",
        "        return {\n",
        "            'input_ids': torch.stack([sample['input_ids'].squeeze(0) for sample in batch]),\n",
        "            'attention_mask': torch.stack([sample['attention_mask'].squeeze(0) for sample in batch]),\n",
        "            'labels': torch.tensor([sample['label'] for sample in batch])\n",
        "        }\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"setfit/20_newsgroups\", split=\"train\")\n",
        "\n",
        "# Create DataLoader with DynamicPaddingCollate\n",
        "train_dataset = MyDataset(dataset, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=DynamicPaddingCollate())\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Set up optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Training loop\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader, 1):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids =batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        # print(outputs)\n",
        "        # check = outputs\n",
        "        # break\n",
        "        #loss =\n",
        "        loss = criterion(outputs, labels)  # Assuming your forward method returns the loss directly\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:  # Print every 10 batches\n",
        "            avg_loss = total_loss / batch_idx\n",
        "            print(f\"Batch {batch_idx}/{len(train_dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    avg_epoch_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ehNWq2TCNif",
        "outputId": "6309e4f8-7fa1-44f7-d214-795cd3965eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2599,  1.7095, -1.8641,  0.4603,  1.0727],\n",
            "        [ 1.3016,  0.2321, -1.3055, -1.0979,  0.3024],\n",
            "        [-0.4994,  0.4463, -2.0962,  1.7518,  1.2974]], requires_grad=True)\n",
            "tensor([2, 4, 0])\n",
            "one tensor(2.9490, grad_fn=<NllLossBackward0>)\n",
            "one tensor(2.9490, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Example of target with class indices\n",
        "# loss = nn.CrossEntropyLoss()\n",
        "# input = torch.randn(3, 5, requires_grad=True)\n",
        "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "# print(input)\n",
        "# print(target)\n",
        "# output = loss(input, target)\n",
        "# print(\"one\",output)\n",
        "# output.backward()\n",
        "# print(\"one\",output)\n",
        "#Example of target with class probabilities\n",
        "# input = torch.randn(3, 5, requires_grad=True)\n",
        "# target = torch.randn(3, 5).softmax(dim=1)\n",
        "# print(target)\n",
        "# output = loss(input, target)\n",
        "# output.backward()\n",
        "# print(\"two\",output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er3P-1WYAfjF",
        "outputId": "e440308d-2f31-462e-a8e3-82cc68828731"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0563, 0.0574, 0.0524, 0.0411, 0.0941, 0.0515, 0.0538, 0.0506, 0.0443,\n",
              "         0.0489, 0.0426, 0.0487, 0.0561, 0.0322, 0.0470, 0.0469, 0.0438, 0.0522,\n",
              "         0.0449, 0.0352],\n",
              "        [0.0521, 0.0482, 0.0492, 0.0369, 0.0977, 0.0582, 0.0564, 0.0400, 0.0496,\n",
              "         0.0411, 0.0519, 0.0455, 0.0738, 0.0366, 0.0482, 0.0340, 0.0500, 0.0406,\n",
              "         0.0575, 0.0323],\n",
              "        [0.0590, 0.0494, 0.0491, 0.0429, 0.0783, 0.0548, 0.0565, 0.0452, 0.0483,\n",
              "         0.0544, 0.0437, 0.0416, 0.0769, 0.0388, 0.0457, 0.0445, 0.0484, 0.0401,\n",
              "         0.0505, 0.0319],\n",
              "        [0.0510, 0.0491, 0.0431, 0.0398, 0.1017, 0.0448, 0.0531, 0.0348, 0.0533,\n",
              "         0.0454, 0.0487, 0.0429, 0.0787, 0.0329, 0.0516, 0.0436, 0.0493, 0.0456,\n",
              "         0.0612, 0.0293],\n",
              "        [0.0507, 0.0465, 0.0489, 0.0376, 0.0945, 0.0496, 0.0660, 0.0351, 0.0511,\n",
              "         0.0464, 0.0428, 0.0453, 0.0753, 0.0330, 0.0497, 0.0458, 0.0557, 0.0445,\n",
              "         0.0510, 0.0305],\n",
              "        [0.0600, 0.0488, 0.0499, 0.0383, 0.0946, 0.0540, 0.0610, 0.0395, 0.0469,\n",
              "         0.0532, 0.0476, 0.0485, 0.0820, 0.0330, 0.0413, 0.0391, 0.0535, 0.0310,\n",
              "         0.0499, 0.0279],\n",
              "        [0.0489, 0.0439, 0.0467, 0.0416, 0.0943, 0.0537, 0.0599, 0.0392, 0.0585,\n",
              "         0.0528, 0.0411, 0.0409, 0.0746, 0.0355, 0.0443, 0.0395, 0.0502, 0.0393,\n",
              "         0.0647, 0.0302],\n",
              "        [0.0450, 0.0477, 0.0466, 0.0373, 0.0984, 0.0515, 0.0618, 0.0377, 0.0500,\n",
              "         0.0607, 0.0452, 0.0451, 0.0809, 0.0334, 0.0414, 0.0428, 0.0504, 0.0415,\n",
              "         0.0537, 0.0289],\n",
              "        [0.0615, 0.0489, 0.0472, 0.0438, 0.0879, 0.0470, 0.0739, 0.0403, 0.0482,\n",
              "         0.0489, 0.0381, 0.0489, 0.0709, 0.0342, 0.0502, 0.0425, 0.0504, 0.0368,\n",
              "         0.0482, 0.0322],\n",
              "        [0.0448, 0.0530, 0.0487, 0.0432, 0.0992, 0.0609, 0.0726, 0.0396, 0.0377,\n",
              "         0.0541, 0.0418, 0.0428, 0.0710, 0.0379, 0.0451, 0.0425, 0.0486, 0.0420,\n",
              "         0.0485, 0.0259],\n",
              "        [0.0649, 0.0406, 0.0445, 0.0385, 0.0790, 0.0498, 0.0620, 0.0404, 0.0427,\n",
              "         0.0515, 0.0569, 0.0477, 0.0775, 0.0360, 0.0456, 0.0455, 0.0603, 0.0382,\n",
              "         0.0502, 0.0283],\n",
              "        [0.0586, 0.0558, 0.0450, 0.0449, 0.0967, 0.0518, 0.0527, 0.0399, 0.0486,\n",
              "         0.0436, 0.0445, 0.0396, 0.0654, 0.0322, 0.0480, 0.0440, 0.0549, 0.0493,\n",
              "         0.0516, 0.0330],\n",
              "        [0.0517, 0.0526, 0.0545, 0.0397, 0.0858, 0.0524, 0.0624, 0.0374, 0.0548,\n",
              "         0.0429, 0.0535, 0.0376, 0.0776, 0.0322, 0.0498, 0.0378, 0.0587, 0.0323,\n",
              "         0.0592, 0.0269],\n",
              "        [0.0441, 0.0534, 0.0498, 0.0380, 0.0882, 0.0566, 0.0672, 0.0426, 0.0539,\n",
              "         0.0511, 0.0457, 0.0451, 0.0812, 0.0321, 0.0451, 0.0381, 0.0420, 0.0387,\n",
              "         0.0560, 0.0314],\n",
              "        [0.0500, 0.0464, 0.0529, 0.0358, 0.0988, 0.0567, 0.0589, 0.0387, 0.0481,\n",
              "         0.0572, 0.0446, 0.0440, 0.0755, 0.0399, 0.0464, 0.0356, 0.0474, 0.0412,\n",
              "         0.0504, 0.0314],\n",
              "        [0.0485, 0.0453, 0.0478, 0.0307, 0.0969, 0.0684, 0.0609, 0.0353, 0.0487,\n",
              "         0.0528, 0.0520, 0.0428, 0.0726, 0.0331, 0.0448, 0.0403, 0.0512, 0.0362,\n",
              "         0.0635, 0.0283],\n",
              "        [0.0415, 0.0415, 0.0552, 0.0385, 0.1105, 0.0488, 0.0568, 0.0371, 0.0511,\n",
              "         0.0551, 0.0448, 0.0455, 0.0663, 0.0346, 0.0438, 0.0508, 0.0494, 0.0432,\n",
              "         0.0521, 0.0335],\n",
              "        [0.0536, 0.0524, 0.0456, 0.0365, 0.1130, 0.0546, 0.0625, 0.0381, 0.0456,\n",
              "         0.0419, 0.0466, 0.0412, 0.0730, 0.0370, 0.0412, 0.0445, 0.0507, 0.0419,\n",
              "         0.0504, 0.0296],\n",
              "        [0.0601, 0.0566, 0.0486, 0.0402, 0.0745, 0.0464, 0.0677, 0.0417, 0.0508,\n",
              "         0.0449, 0.0466, 0.0454, 0.0792, 0.0407, 0.0434, 0.0389, 0.0508, 0.0341,\n",
              "         0.0585, 0.0310],\n",
              "        [0.0548, 0.0545, 0.0464, 0.0322, 0.0898, 0.0498, 0.0599, 0.0493, 0.0480,\n",
              "         0.0460, 0.0446, 0.0438, 0.0709, 0.0375, 0.0463, 0.0431, 0.0548, 0.0392,\n",
              "         0.0543, 0.0347],\n",
              "        [0.0451, 0.0517, 0.0393, 0.0332, 0.0963, 0.0542, 0.0732, 0.0400, 0.0521,\n",
              "         0.0539, 0.0544, 0.0392, 0.0767, 0.0298, 0.0512, 0.0436, 0.0478, 0.0368,\n",
              "         0.0509, 0.0306],\n",
              "        [0.0622, 0.0447, 0.0520, 0.0373, 0.1000, 0.0474, 0.0619, 0.0359, 0.0552,\n",
              "         0.0447, 0.0386, 0.0469, 0.0778, 0.0366, 0.0438, 0.0359, 0.0523, 0.0386,\n",
              "         0.0572, 0.0310],\n",
              "        [0.0472, 0.0482, 0.0536, 0.0370, 0.0985, 0.0531, 0.0591, 0.0386, 0.0562,\n",
              "         0.0642, 0.0459, 0.0416, 0.0739, 0.0279, 0.0354, 0.0405, 0.0573, 0.0413,\n",
              "         0.0543, 0.0262],\n",
              "        [0.0605, 0.0452, 0.0480, 0.0341, 0.0792, 0.0537, 0.0726, 0.0402, 0.0533,\n",
              "         0.0601, 0.0409, 0.0432, 0.0679, 0.0405, 0.0418, 0.0439, 0.0532, 0.0436,\n",
              "         0.0503, 0.0278],\n",
              "        [0.0536, 0.0495, 0.0454, 0.0375, 0.0946, 0.0525, 0.0646, 0.0399, 0.0558,\n",
              "         0.0494, 0.0493, 0.0371, 0.0759, 0.0343, 0.0384, 0.0406, 0.0534, 0.0401,\n",
              "         0.0575, 0.0307],\n",
              "        [0.0547, 0.0504, 0.0533, 0.0372, 0.0924, 0.0515, 0.0546, 0.0311, 0.0509,\n",
              "         0.0490, 0.0424, 0.0487, 0.0835, 0.0338, 0.0414, 0.0393, 0.0494, 0.0419,\n",
              "         0.0670, 0.0274],\n",
              "        [0.0594, 0.0434, 0.0421, 0.0388, 0.0972, 0.0593, 0.0551, 0.0353, 0.0609,\n",
              "         0.0486, 0.0500, 0.0445, 0.0705, 0.0277, 0.0487, 0.0408, 0.0504, 0.0362,\n",
              "         0.0573, 0.0337],\n",
              "        [0.0481, 0.0426, 0.0556, 0.0372, 0.1081, 0.0580, 0.0579, 0.0331, 0.0498,\n",
              "         0.0455, 0.0423, 0.0469, 0.0739, 0.0342, 0.0402, 0.0499, 0.0436, 0.0418,\n",
              "         0.0546, 0.0366],\n",
              "        [0.0559, 0.0521, 0.0506, 0.0331, 0.0884, 0.0544, 0.0600, 0.0437, 0.0492,\n",
              "         0.0445, 0.0493, 0.0446, 0.0816, 0.0360, 0.0452, 0.0393, 0.0518, 0.0353,\n",
              "         0.0524, 0.0326],\n",
              "        [0.0508, 0.0458, 0.0600, 0.0349, 0.0944, 0.0487, 0.0590, 0.0410, 0.0522,\n",
              "         0.0572, 0.0461, 0.0453, 0.0849, 0.0361, 0.0392, 0.0416, 0.0509, 0.0374,\n",
              "         0.0472, 0.0272],\n",
              "        [0.0496, 0.0510, 0.0462, 0.0365, 0.1031, 0.0544, 0.0548, 0.0423, 0.0542,\n",
              "         0.0526, 0.0473, 0.0460, 0.0719, 0.0339, 0.0408, 0.0376, 0.0419, 0.0423,\n",
              "         0.0561, 0.0375],\n",
              "        [0.0514, 0.0488, 0.0572, 0.0355, 0.0942, 0.0562, 0.0554, 0.0337, 0.0495,\n",
              "         0.0610, 0.0475, 0.0413, 0.0772, 0.0289, 0.0440, 0.0377, 0.0541, 0.0400,\n",
              "         0.0544, 0.0320]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmJzqLGIAoSB",
        "outputId": "8e230dd2-f6fd-49fb-f158-3212be320ef1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
              "        12,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# magic = torch.argmax(outputs, dim = 1)\n",
        "# magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c6DeXuo0AF8Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the model\n",
        "model.save_model(\"D:/Studies/DL/vsccode/models/my_custom_bert_modelv2.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TESTING CHECK TOMORROW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "20NryXzZADIv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3, 0, 3, 0, 2, 3, 2, 4, 0, 3, 3, 4, 2, 3, 2, 3, 4, 3, 2, 2, 2, 3, 3, 3,\n",
            "         0, 3, 2, 3, 2, 4, 3, 3, 3, 4, 3, 1, 3, 4, 4, 3, 4, 4, 3, 4, 2, 3, 2, 3,\n",
            "         2, 3, 3, 4, 3, 3, 3, 1, 4, 4, 4, 0, 1, 1, 2, 3, 2, 4, 3, 0, 3, 3, 3, 0,\n",
            "         2, 1, 3, 0, 1, 0, 4, 3, 1, 2, 0, 2, 4, 0, 0, 1, 3, 2, 4, 4, 3, 3, 3, 2,\n",
            "         3, 3, 3, 1, 4, 3, 0, 2, 3, 3, 3, 0, 2, 4, 2, 2, 2, 3, 3, 3, 4, 0, 2, 3,\n",
            "         3, 4, 4, 2, 2, 3, 4, 3, 3, 0, 0, 2, 2, 3, 1, 4, 1, 3, 4, 3, 3, 0, 3, 4,\n",
            "         2, 0, 2, 4, 4, 1, 1, 0, 3, 4, 3, 4, 4, 3, 0, 3, 3, 2, 4, 3, 4, 3, 3, 3,\n",
            "         3, 4, 3, 2, 2, 3, 2, 3, 0, 3, 3, 4, 4, 3, 3, 3, 2, 3, 4, 0, 2, 4, 2, 4,\n",
            "         3, 4, 2, 3, 0, 2, 4, 1, 1, 4, 0, 0, 4, 3, 2, 4, 4, 4, 0, 3, 3, 4, 3, 3,\n",
            "         2, 3, 4, 1, 2, 3, 3, 3, 3, 0, 3, 4, 2, 4, 4, 2, 2, 4, 3, 2, 1, 2, 4, 3,\n",
            "         2, 3, 3, 2, 3, 1, 0, 4, 4, 4, 3, 2, 1, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3,\n",
            "         1, 3, 0, 0, 1, 4, 3, 3, 3, 3, 0, 3, 3, 2, 3, 3, 2, 3, 0, 3, 3, 2, 2, 2,\n",
            "         4, 3, 3, 3, 3, 3, 2, 3, 4, 3, 3, 3, 4, 4, 0, 3, 2, 3, 3, 3, 4, 4, 4, 3,\n",
            "         3, 1, 1, 4, 0, 1, 3, 3, 3, 0, 3, 3, 2, 0, 2, 3, 3, 4, 3, 3, 0, 2, 2, 3,\n",
            "         3, 2, 3, 4, 3, 3, 3, 2, 0, 4, 4, 4, 1, 3, 2, 3, 3, 2, 3, 0, 4, 0, 3, 0,\n",
            "         3, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 4, 3, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 4,\n",
            "         2, 2, 3, 2, 3, 4, 2, 3, 2, 3, 3, 0, 3, 4, 0, 1, 3, 3, 4, 4, 4, 4, 1, 3,\n",
            "         1, 4, 4, 2, 4, 2, 4, 3, 3, 0, 4, 0, 3, 2, 4, 3, 2, 4, 4, 3, 3, 3, 4, 0,\n",
            "         3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 4, 2, 4, 2, 3, 3, 3,\n",
            "         2, 0, 0, 1, 4, 3, 4, 3, 2, 4, 3, 3, 4, 4, 4, 2, 3, 4, 4, 3, 1, 4, 4, 3,\n",
            "         0, 3, 3, 3, 4, 0, 1, 3, 3, 1, 3, 4, 3, 4, 0, 3, 0, 1, 0, 2, 2, 4, 4, 3,\n",
            "         3, 3, 3, 3, 4, 3, 2, 3]])\n"
          ]
        }
      ],
      "source": [
        "# # Load the model for predictions\n",
        "# loaded_model = Bert.load_model(config_dict, \"D:/Studies/DL/vsccode/models/my_custom_bert_modelv1.pth\")\n",
        "\n",
        "# # Perform predictions\n",
        "# sentence = \"I like computers\"\n",
        "# input_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
        "# output = loaded_model(input_ids)\n",
        "# print(torch.argmax(F.softmax(output[0], dim=1), dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.0070, -0.1704, -0.0670,  ...,  0.1719,  0.6466,  0.5834],\n",
              "         [-0.4745, -0.3692, -0.0863,  ...,  0.1384,  0.7785,  0.5543],\n",
              "         [-0.5119, -0.3893, -0.0829,  ...,  0.1456,  0.8330,  0.5947],\n",
              "         [-0.4829, -0.3874, -0.0650,  ...,  0.1734,  0.8240,  0.5547],\n",
              "         [-0.4764, -0.3769, -0.1003,  ...,  0.1372,  0.7869,  0.0832]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3UyqatN5buz",
        "outputId": "2d2ba3a9-3425-4927-aab4-c26aa5faa8fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0868, 0.0667, 0.0448, 0.0425, 0.0855, 0.0469, 0.0439, 0.0399, 0.0453,\n",
              "        0.0427, 0.0820, 0.0357, 0.0340, 0.0547, 0.0405, 0.0406, 0.0423, 0.0527,\n",
              "        0.0329, 0.0395], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "check[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_tKQzbI5vQf",
        "outputId": "32dbd1c6-e2aa-4a79-9438-9cb86f4674d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = torch.argmax(check[0])\n",
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WcxT3HW6Cbp",
        "outputId": "8ec65fb7-9be6-4582-a81d-621535b04f52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.3700,  0.4549,  0.0396,  0.1715,  0.2064,  0.1312,  0.3936, -0.0831,\n",
              "         0.2345, -0.2406, -0.2414,  0.3022, -0.1611, -0.3026, -0.0525,  0.0938,\n",
              "         0.2632, -0.0264,  0.3938,  0.2777], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "check[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFxJolfz6O__",
        "outputId": "38c8913d-4cf6-4d7a-f0a5-ae9745b1b444"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([17,  4,  9, 13, 19, 16,  5,  1,  6,  6, 12,  3,  4, 19, 10,  4,  1, 14,\n",
              "         5, 13, 19, 17,  0, 19, 16, 12,  0, 10,  6,  3,  7,  0])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.save(model.state_dict(), 'model_weights.pth')\n",
        "# model = MyModel()  # Make sure this is the same model architecture\n",
        "# model.load_state_dict(torch.load('model_weights.pth'))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxPJfvlb_n5x"
      },
      "outputs": [],
      "source": [
        "# Access the 19th row in the \"text\" column\n",
        "text_19th_row = dataset['train']['text'][18]\n",
        "\n",
        "print(text_19th_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "MyLKjtn48Iwk",
        "outputId": "87f1b2c2-7d2e-4f85-e129-86b509b4b36e"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'input_ids' is an invalid keyword argument for print()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b03af2fa4485>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'input_ids' is an invalid keyword argument for print()"
          ]
        }
      ],
      "source": [
        "for batch_idx, batch in enumerate(train_dataloader, 1):\n",
        "  print(**batch)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-vGz43W8KFf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
